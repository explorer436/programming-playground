* Things that need research

** High level definitions

*** Kafka

    Basically an event streaming platform. It enables users to collect, store, and process data to build real-time event-driven applications. It’s written in Java and Scala, but you don’t have to know these to work with Kafka. There’s also a Python API.

*** Kafka topic

   A category to which records are published. Imagine you had a large news site — each news category could be a single Kafka topic.

   A stream of data within the kafka cluster.

   To use an anology, they are like tables in a database (without all the constraints). There is no data verification.

   You can have as many topics in a cluster as you want. e.g. logs, purchases, tweets, truck gps, etc. Each topic is identified by its name.

   They support any kind of message format. e.g. json, avro, text file, binary, etc.

   The sequence of messages is called a data stream.

   You cannot query topics. Instead, use kafka producers to send data to topics and kafka consumers to read data from topics.

   Topics are immutable. Once data is written to a partition, it cannot be changed. You cannot delete it. You cannot update it.

   Data is kept in the topic only for a limited time (default is one week - but this is configurable).

**** Partitions and offsets

     Topics are split into ~partitions~ (e.g. 100 partitions). The messages sent to kafka topics end up in these partitions. Messages within each partitions are ordered.

     #+begin_src

                    |-- Partition 0 |0|1|2|3|4|5|6|7|8|9|10|11|12| 
                    |
     kafka topic -- |-- Partition 1 |0|1|2|3|4|5|6|7|8|
                    |
                    |-- Partition 2 |0|1|2|3|4|5|6|7|8|9|10|

     #+end_src

     Each message within a partition gets an incremental id. This id is called ~offset~.

     Offset only have a meaning for a specific partition. e.g. offset 3 in partition 0 does not represent the same data as offset 3 in partition 1. Offsets are not re-used even if previous messages have been deleted.

     Order of messages is guaranteed only within a partition but not across partitions.

     Data is assigned randomly to a partition unless a key is provided.

e.g. scenario for a topic:

Lets say you have a fleet of trucks.
Each truck reports its GPS position to kafka.
Each truck will send a message to kafka every 20 seconds.
Each message will contain the truck ID and the truck position (latitude and longitude).
You can have a topic "truck_gps" that contains the positions of all trucks.
You can choose to create a topic with 10 partitions (arbitrary number).
Consumers can consume data from this topic and do whatever they want to do with the same stream of data e.g. one consumer can create a location dashboard; another consumer can set up notification services based on this data;
**** Topic replication factor

     Topics in kafka should have a replication factor > 1 (usually between 2 and 3)

     That way, if a broker is down, another broker can serve the data (from a copy of the topic)

     e.g. TopicA with 2 partitions and replication factor = 2

     #+begin_src 

     Broker101                   Broker102                          Broker103

     TopicA    ------            TopicA----------replication--------TopicA
     Partition0      |           Partition1                         Partition1
                     |
                     |
                replication
                     |
                      ---------- TopicA
                                 Partition0

     #+end_src

     If Broker102 goes down, Broker101 and Broker103 can still serve data.

**** Concept of Leader for a partition

     At any time, only one broker can be a leader for a given partition.

     Producers can only send data to the broker that is the leader of a partition.

     The other brokers will replicate the data.

     Therefore, each partition has one leader and multiple ISRs (in-sync replica)

     e.g.

     Broker101 is leader for TopicA Partition0

     Broker102 is leader for TopicA Partition1

**** Default producer and consumer behavior with leaders

     Kafka producers can only write to the leader broker for a partition.

     Kafka consumers by default will read from the leader broker for a partition.

**** Kafka Consumers Replica fetching (newer kafka versions)

     Since kafka 2.4, it is possible to configure consumers to read from the closest replica.

     This may help improve latency, and also decrease network costs if using the cloud.

**** Kafka Topic durability

     For a topic replication factor = 3, topic data durability can withstand 2 brokers loss.

     If replication factor = n, you can loose n-1 brokers and still recover your data.
     
*** Kafka producer

   An application (a piece of code) you write to get data to Kafka topics (which are made of partitions).

   Producers know (in advance) to which partition they should write data to (and which kafka broker has the partition).

   In case of kafka broker failures, Producers will know how to automatically recover.

**** Producer acknowledgements (ack)

     Kafka Producer Acks Deep Dive - https://www.conduktor.io/kafka/kafka-producer-acks-deep-dive

     Producers send data into kafka brokers.

     Producers can choose to receive acknowledgements of data writes:
     - acks=0: Producer will not wait for acknowledgement (possible data loss)
     - acks=1: Producer will wait for leader acknowledgement (limited data loss)
     - acks=all: Leader + replicas acknowledgement (no data loss)

**** Message keys

   Producers can choose to send a key with the message (string, number, binary, etc.)

   If key==null, the data will be sent to the partitions in the topic in a round robin fashion. This is how load balancing is achieved.

   If key != null, then all the messages for that key will always go to the same partition (hashing). We use this when we need to ensure the message ordering. e.g. If you prefer to receive the location of each individual truck in order, we need to use partitions to ensure ordering. And the producer can use the truck id as the key of the messages.

   #+begin_src 
   
                    |-- Partition 0 truck_id_123, truck_id_234
                    |
     kafka topic -- |-- Partition 1 truck_id_456
                    |
                    |-- Partition 2 truck_id_789
  
   #+end_src

   The data for truck_id_123 will always be in Partition 0.

*** Kafka messages

    Anatomy of a kafka message

    #+begin_src

    |--------------|--------------------|
    | key - binary | value - binary     |
    | (can be null | (can be null)      |
    |--------------|--------------------|
    |     Compression Type              |
    |(none, gzip, snappy, lz4, zstd)    |
    |--------------|--------------------|
    |      Headers (optional)           |
    |        key - value                |
    |        key - value                |
    |--------------|--------------------|
    |    Partition + Offset             |
    |--------------|--------------------|
    |    Timestamp (system or user set) |
    |--------------|--------------------|

    #+end_src

**** Kafka message serializer

     kafka only accepts bytes as an input from producers and sends bytes out as an output to consumers.

     Message serialization means transforming objects/data into bytes.

     They are used on the value and the key.

     e.g. key object = 123 (truck id), value object (latitule + longitude of the truck)

     KeySerializer (Integer serializer) will transform the key into bytes.

     ValueSerializer (String serializer) will transform the value into bytes.

     kafka producers come with standard built-in serializers
     - String (includes JSON)
     - Integer, Float
     - Avro
     - Protobuf

**** Kafka message Key Hashing

     A kafka partitioner is a code logic that takes a record and determines to which partition to send it to.

     #+begin_src 
      --------        --------      ----------------------------      --------------------      -------------
     | Record |  ->  | send() | -> | Producer Partitioner Logic | -> | Assign partition 1 | -> | Partition 1 |
      --------        --------      ----------------------------      --------------------      -------------
     #+end_src

     Key hashing is the process of determining the mapping of a key to a partition.

     In the default kafka partitioner, the keys are hashed using the murmer2 algorithm, with the formula the curious:

     #+begin_src 
     targetPartition = Math.abs(Utils.murmer2(keyBytes)) % (numPartitions - 1)
     #+end_src

     So, producers are the ones to decide the partition in which the message ends up being placed.

*** Kafka consumer

   A program you write to get data out of Kafka. Sometimes a consumer is also a producer, as it puts data elsewhere in Kafka.

   Consumers read data from a topic (identified by name) - pull model.

   Producers use the push model and consumers use the pull model.

   #+begin_src 
                                                                ------------
   Topic A - Partition 0 |-- |0|1|2|3|4|5|6|7|8|9|10|11|12| -> | Consumer 1 |
                                                                ------------

                                                                ------------
   Topic A - Partition 0 |-- |0|1|2|3|4|5|6|7|8|9|10|11|12| -> |            |
                         |                                     | Consumer 2 |
   Topic A - Partition 0 |-- |0|1|2|3|4|5|6|7|8|9|10|11|12| -> |            |
                                                                ------------
  
   #+end_src

   Consumers automatically know which broker to read from.

   In case of broker failures, consumers know how to recover.

   Data is read in order from low to high offset "within each partition".

   There is no order guaranteed across partitions.

**** Consumer Deserializer

     Deserializer indicates how to transform bytes into objects/data.

     In a way, this does the opposite function of what the producer serializer does.

     The serialization/deserialization type (datatype) must not change during a topic lifecycle (If necessary, create a new topic instead).

     e.g. changing the key type from Integer to String is not allowed for a topic (because the consumers will be looking for that specific datatype)
     
*** Consumer groups

    All the consumers in an application read data as a consumer group.

    e.g. You have a kafka topic with 5 partitions - Partition0, Partition1, Partition2, Partition3, Partition4
    Then, you have a consumer group with three consumers - Consumer1, Consumer2, Consumer3

    Each consumer within a group reads from exclusive partitions.

    Consumer1 reads from Partition0 and Partition1.

    Consumer2 reads from Partition2 and Partition3.

    Consumer3 reads from Partition4.

**** What if you have too many consumers?

     If you have more consumers than partitions, some consumers will be inactive.

     e.g.

     Scenario1:
     Topic A - Partition0 - Consumer0 is reading from this partition.

     Topic A - Partition1 - Consumer1 is reading from this partition.

     Topic A - Partition2 - Consumer2 is reading from this partition.          

     Scenario 2:
     Topic A - Partition0 - Consumer0 is reading from this partition.

     Topic A - Partition1 - Consumer1 is reading from this partition.

     Topic A - Partition2 - Consumer2 is reading from this partition.

     And there is an extra consumer called Consumer4
	       
     In Scenario 2, Consumer4 will be inactive.

**** Multiple consumes on one topic

     In Apache Kafka, it is acceptable to have multiple consumer groups on the same topic.

     e.g.

     Topic A - Partition0,                 Partition1,               Partition2
     
      | consumer-group-application-1 | -> Consumer1 reads from Partition0 and Partition1 and Consumer2 reads from Partition2.

      | consumer-group-application-2 | -> Consumer1 reads from Partition0. Consumer2 reads from Partition1. Consumer3 reads from Partition2.

      | consumer-group-application-3 | -> Consumer1 reads from Partition0, Partition1 and Partition2.      

      As we can see, we have multiple consumer groups reading from the same topic. Each partition will have multiple readers. But within a consumer group, only one consumer is assigned to one partition in a topic.

      Why would we have multiple consumer groups?

      Going back to the truck example, we have one consumer-group that needs to create a location dashboard; and another consumer-group that need to set up notification services based on this data;

      To create distinct consumer groups, use the consumer property ~group.id~

      In these groups, you can define ~consumer offsets~

      Kafka stores the offsets at which a consumer group has been reading.

      The offsets committed are in a Kafka topic named __consumer_offsets (two underscores at the beginning because it is an internal kafka topic).

      When a consumer in a group has processed data received from kafka, it should be periodically committing the offsets (the kafka broker will write to __consumer_offsets, not the group itself).

      If a consumer dies, it will be able to read back from where it left off - thanks to the committed consumer offsets.

**** Delivery semantics for consumers

     By default, Java Consumers will automatically commit offsets (at least once).

     If you choose to commit manually, there are 3 delivery semantics.
     - At least once (usually preferred)
       - Offsets are committed after the message is processed
       - If the processing goes wrong, the message will be read again
       - This can result in duplicate processing of messages. Make sure your processing is idempotent (i.e. processing the messages again will not impact your systems)
     - At most once
       - Offsets are committed as soon as messages are received.
       - If the processing goes wrong, some messageswill be lost (they will not be read again)
     - Exactly once
       - For kafka => kafka workflows: use the Transactional API (easy with Kafka Streams API)
       - For kafka => External System workflows: use and idempotent consumer
	 
*** Kafka broker

    A single Kafka Cluster is made of or composed of multiple Brokers(They are just servers - but in kafka, they are called brokers because they receive and send data). They handle producers and consumers and keeps data replicated in the cluster.

    Each broker is identified with its ID (integer).
    - e.g. Broker101, Broker102, Broker103

    Each broker contains certain topic partitions.
    - It means your data is going to be distributed into multiple brokers.

    After connecting to any broker (called a bootstrap broker), you will be connected to the entire cluster (Kafka clients have smart mechanics for that).

    A good number to get started is 3 brokers.
    - Some big clusters have over 100 brokers.

**** How are brokers and topics related?

     Example Scenario:
     - TopicA with 3 partitions and TopicB with 2 partitions
     - Three brokers Broker101, Broker102 and Broker103

       #+begin_src 
       | Broker101           | Broker102           | Broker103
       | TopicA - Partition0 | TopicA - Partition2 | TopicA - Partition1
       | TopicB - Partition1 | TopicB - Partition0 |
       #+end_src

       Data is distributed.
       - This distribution lets kafka scale horizontally.

       Broker103 does not have any TopicB data.

**** Kafka broker discovery

     Every kafka broker is also called a "bootstrap server".

     That means that you only need to connect to one broker, and the Kafka clients will know how to be connected to the entire cluster (smart clients)

     When a client initiates a request to a broker, a connection will be first established with that broker and a list of all the other brokers from that cluster is returned from that broker.

     Each broker knows about all brokers, topics and partitions (metadata).
     
*** Zookeeper

   - Used to manage a Kafka cluster, track node status, and maintain a list of topics and messages.
   - Zookeeper manages brokers (keeps a list of them).
   - Zookeeper helps in performing leader election for partitions.
   - Zookeeper sends notifications to Kafka in case of changes (e.g. new topic, broker dies, broker comes up, delete topics, etc.)

   Kafka 2.x cannot work without Zookeeper.

   kaf 3.x can work without Zookeeper (KIP-500) - using Kafka Raft instead.

   kafka 4.x will not have Zookeeper.

   Kafka version 2.8.0 introduced early access to a Kafka version without Zookeeper, but it’s not ready yet for production environments.

   Zookeeper, by design, operates with an odd number of servers (1,3,5,7).

   Zookeeper has a leader (writes) the rest of the servers are followers (reads).

   Zookeeper does not store consumer offsets with Kafka > v0.10

**** Zookeeper Cluster (ensemble)

     #+begin_src

     Zookeeper Server 1        Zookeeper Server 2         Zookeeper Server 3
     (follower)                (Leader)                   (follower)
         |                             |                     |
         |                             |                     |
      -------------               ----------                 |
     |             |             |          |                |
     |             |             |          |                |
    Broker1     Broker2        Broker3   Broker4           Broker5
  
     #+end_src

**** Should you use Zookeeper?

     With kafka brokers?
     - Yes, until kafka 4.0 is out while waiting for kafka without Zookeeper to be production ready.

     With kafka clients?
     - Over time, the kafka clients and CLI have been migrated to leverage the broker as a connection endpoint instead of Zookeeper
     - Since kafka 0.10, consumers store offset in Kafka and Zookeeper and must not connect to Zookeeper as it is deprecated.
     - Since kafka 2.2, the kafka-topics.sh CLI command references kafka brokers and not Zookeeper for topic management (creation, deletion, etc.) and the Zookeeper CLI argument is deprecated.
     - All the APIs and commands that were previously leveraging Zookeeper are migrated to use Kafka instead, so that when clusters are migrated to be without Zookeeper, the change is invisible to clients.
     - Zookeeper is less secure than kafka. Therefore, Zookeeper ports should only be opened to allow traffic from kafka brokers, and not kafka clients
     - Therefore, to be a great modern-day kafka developer, never use Zookeeper as a configuration in your kafka clients, and other programs that connect to kafka.

**** Kafka KRaft

     - In 2020, the Apache Kafka project started to work to remove the Zookeeper dependency from it (KIP-500)
     - Zookeeper shows scaling issues when Kafka clusters have > 100,000 partitions.
     - By removing Zookeeper, Apache Kafka can
       - Scale to millions of partitions, and becomes easier to maintain and set-up
       - Improve stability, makes it easier to monitor, support and administer
       - Single security model for the whole system
       - Single process to start with Kafka
       - Faster controller shutdown and recovery time
     - Kafka 3.x now implements the Raft protocal (KRaft) in order to replace Zookeeper

** Tools   

*** Connecting to Kafka for administration

See docker-compose-wurstmeister.yml for these details

*** GUI Tool

    1. GUI tool to explore kafka setup: https://www.kafkamagic.com/

    2. Use this tool for GUI views of local kafka cluster: https://github.com/redpanda-data/console

       Command to run it: ~docker run -p 8080:8080 -e KAFKA_BROKERS=host.docker.internal:9092 docker.redpanda.com/vectorized/console:latest~

       It runs at port 8080

       And use the browser: http://localhost:8080/topics

    3. There are more tools listed here: https://dev.to/dariusx/recommend-a-simple-kafka-ui-tool-5gob

** Helpful resources

https://www.conduktor.io/kafka - This is very good.

https://medium.com/@TimvanBaarsen/apache-kafka-cli-commands-cheat-sheet-a6f06eac01b

https://www.gentlydownthe.stream/ - A cute children’s book explaining Kafka.

** Use cases   

   Kafka use cases: https://kafka.apache.org/powered-by

** Additional research  

Problems with kafka streams : https://dzone.com/articles/problem-with-kafka-streams-1?fromrel=true

Kafka Racing: Know the Circuit : https://dzone.com/articles/kafka-racing-know-the-circuit

1. springboot Apache Kafka messaging
1. spring-cloud-stream
1. spring-cloud-stream-binder-kafka
1. What are kafka containers?
1. Cloudant - kafka streams or queues
1. How to publish topic from lambda to kafka stream and from kafka stream to lambda?
