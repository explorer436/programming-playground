* Stream Processing with Apache Kafka

In this guide, we develop three Spring Boot applications that use Spring Cloud Stream's support for Apache Kafka and deploy them to Cloud Foundry, Kubernetes, and your local machine. In another guide, we deploy these applications by using Spring Cloud Data Flow. By deploying the applications manually, you get a better understanding of the steps that Data Flow can automate for you.

** Development

We create three Spring Cloud Stream applications that communicate using Kafka.

The scenario is a cell phone company creating bills for its customers. Each call made by a user has a duration and an amount of data used during the call. As part of the process to generate a bill, the raw call data needs to be converted to a cost for the duration of the call and a cost for the amount of data used.

The call is modeled by using the UsageDetail class, which contains the duration of the call and the amount of data used during the call. The bill is modeled by using the UsageCostDetail class, which contains the cost of the call ( costCall ) and the cost of the data ( costData ). Each class contains an ID ( userId ) to identify the person making the call.

The three streaming applications are as follows:

1. The Source application (named UsageDetailSender ) generates the user's call duration and amount of data used per userId and sends a message containing the UsageDetail object as JSON.
1. The Processor application (named UsageCostProcessor ) consumes the UsageDetail and computes the cost of the call and the cost of the data per userId . It sends the UsageCostDetail object as JSON.
1. The Sink application (named UsageCostLogger ) consumes the UsageCostDetail object and logs the cost of the call and the cost of the data.

** UsageDetailSender source

1. Create a new Maven project with a Group name of io.spring.dataflow.sample and an Artifact name of usage-detail-sender-kafka .
2. In the Dependencies text box, type Kafka to select the Kafka binder dependency.
3. In the Dependencies text box, type Cloud Stream to select the Spring Cloud Stream dependency.
4. In the Dependencies text box, type Actuator to select the Spring Boot actuator dependency.
5. If your target platform is Cloud Foundry , type Cloud Connectors to select the Spring Cloud Connector dependency.
6. Click the Generate Project button.

*** Business Logic

Now we can create the code required for this application. To do so:

1. Create a UsageDetail class in the io.spring.dataflow.sample.usagedetailsender package with content that resembles UsageDetail.java. This UsageDetail model contains userId , data , and duration properties.
2. Create the UsageDetailSender class in the io.spring.dataflow.sample.usagedetailsender package with content that resembles the following:

#+begin_src 
package io.spring.dataflow.sample.usagedetailsenderkafka;

import java.util.Random;
import java.util.function.Supplier;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class UsageDetailSender {

  private String[] users = {"user1", "user2", "user3", "user4", "user5"};

  /**
   * Configuring the UsageDetailSender application:
   * When configuring the producer application, we need to set the producer binding destination (Kafka topic) where the producer publishes the data. 
   * The default producer output binding for the above method is going to be sendEvents-out-0 
   * (method name followed by the literal -out-0 where 0 is the index). 
   * If the application does not set a destination, Spring Cloud Stream will use this same binding name as the output destination (Kafka topic). 
   * However, in our case, we neither want this default binding name used by Spring Cloud Stream nor the destination name. 
   * We want to use the binding name as output and provide a custom destination. 
   * 
   * See application.properties
   */
  @Bean
	public Supplier<UsageDetail> sendEvents() {
		return () -> {
			UsageDetail usageDetail = new UsageDetail();
			usageDetail.setUserId(this.users[new Random().nextInt(5)]);
			usageDetail.setDuration(new Random().nextInt(300));
			usageDetail.setData(new Random().nextInt(700));
			return usageDetail;
		};
	}
}
  
#+end_src
   
This is a simple Configuration class with a single bean that returns a java.util.function.Supplier . Spring Cloud Stream, behind the scenes will turn this Supplier into a producer. By default, the supplier will be invoked every second. On each invocation, the supplier method sendEvents constructs a UsageDetail object.

*** Configuring the UsageDetailSender application

When configuring the producer application, we need to set the producer binding destination (Kafka topic) where the producer publishes the data. The default producer output binding for the above method is going to be sendEvents-out-0 (method name followed by the literal -out-0 where 0 is the index). If the application does not set a destination, Spring Cloud Stream will use this same binding name as the output destination (Kafka topic). However, in our case, we neither want this default binding name used by Spring Cloud Stream nor the destination name. We want to use the binding name as output and provide a custom destination.

In src/main/resources/application.properties , you can add the following properties to override:

#+begin_src 
spring.cloud.stream.function.bindings.sendEvents-out-0=output
spring.cloud.stream.bindings.output.destination=usage-detail  
#+end_src

The first property will override the default binding name to output and the second one will set destination on that binding.

*** Building

Now we can build the Usage Detail Sender application. In the usage-detail-sender directory, use the following command to build the project using maven:

~mvn clean package~

*** Testing

Spring Cloud Stream provides a test binder to test an application. Following are the maven coordinates for this artifact.

#+begin_src 
<dependency>
<groupId>org.springframework.cloud</groupId>
<artifactId>spring-cloud-stream</artifactId>
<type>test-jar</type>
<classifier>test-binder</classifier>
<scope>test</scope>
</dependency>  
#+end_src

Instead of the Kafka binder, the tests use the Test binder to trace and test your application's outbound and inbound messages. The Test binder provides abstractions for output and input destinations as OutputDestination and InputDestination . Using them, you can simulate the behavior of actual middleware based binders.

To unit test this UsageDetailSender application, add the following code in the UsageDetailSenderApplicationTests class:

#+begin_src 
package io.spring.dataflow.sample.usagedetailsender;

import io.spring.dataflow.sample.UsageDetail;
import org.junit.jupiter.api.Test;
import org.springframework.boot.WebApplicationType;
import org.springframework.boot.builder.SpringApplicationBuilder;
import org.springframework.cloud.stream.binder.test.OutputDestination;
import org.springframework.cloud.stream.binder.test.TestChannelBinderConfiguration;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.messaging.Message;
import org.springframework.messaging.converter.CompositeMessageConverter;
import org.springframework.messaging.converter.MessageConverter;

import static org.assertj.core.api.Assertions.assertThat;

public class UsageDetailSenderApplicationTests {

    @Test
	public void contextLoads() {
	}

	@Test
	public void testUsageDetailSender() {
		try (ConfigurableApplicationContext context = new SpringApplicationBuilder(
				TestChannelBinderConfiguration
						.getCompleteConfiguration(UsageDetailSenderApplication.class))
				.web(WebApplicationType.NONE)
				.run()) {

			OutputDestination target = context.getBean(OutputDestination.class);
			Message<byte[]> sourceMessage = target.receive(10000);

			final MessageConverter converter = context.getBean(CompositeMessageConverter.class);
			UsageDetail usageDetail = (UsageDetail) converter
					.fromMessage(sourceMessage, UsageDetail.class);

			assertThat(usageDetail.getUserId()).isBetween("user1", "user5");
			assertThat(usageDetail.getData()).isBetween(0L, 700L);
			assertThat(usageDetail.getDuration()).isBetween(0L, 300L);
		}
	}
}
#+end_src

1. The contextLoads test case verifies the application starts successfully.
1. The testUsageDetailSender test case uses the test binder to receive messages from the output destination where the supplier publishes messages to.

** UsageCostProcessor Processor   

1. Create a new Maven project with a Group name of io.spring.dataflow.sample and an Artifact name of usage-cost-processor-kafka .
1. In the Dependencies text box, type kafka to select the Kafka binder dependency.
1. In the Dependencies text box, type cloud stream to select the Spring Cloud Stream dependency.
1. In the Dependencies text box, type Actuator to select the Spring Boot actuator dependency.
1. If your target platform is Cloud Foundry , type Cloud Connectors to select the Spring Cloud Connector dependency.
1. Click the Generate Project button.

*** Business Logic

1. Create the UsageDetail class in the io.spring.dataflow.sample.usagecostprocessor with content that resembles UsageDetail.java. The UsageDetail class contains userId , data and, duration properties
1. Create the UsageCostDetail class in the io.spring.dataflow.sample.usagecostprocessor package with content that resembles UsageCostDetail.java. This UsageCostDetail class contains userId , callCost , and dataCost properties.
1. Create the UsageCostProcessor class in the io.spring.dataflow.sample.usagecostprocessor package that receives the UsageDetail message, computes the call and data cost and sends a UsageCostDetail message. The following listing shows the source code:

#+begin_src 
package io.spring.dataflow.sample.usagecostprocessorkafka;

import java.util.function.Function;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/**
 * 
 * We are providing a bean that returns a java.util.function.Function that consumes a UsageDetail as input and publishes a UsageCostDetail as ouptut.
 * 
 * Configuring the UsageCostProcessor Application:
   When configuring this processor application, we need to set both the input and output destinations (Kafka topics). 
   By default, Sprig Cloud Stream uses binding names as processUsageCost-in-0 and processUsageCost-out-0 which becomes the topic names unless the application overrides them. 
   However, we don't want these defaults but rather we would want to make them more descriptive. 
   We want to use the binding name as input and output and provide custom destinations on them.

   See application.properties
 *
 */
@Configuration
public class UsageCostProcessor {

	private double ratePerSecond = 0.1;

	private double ratePerMB = 0.05;

	@Bean
	public Function<UsageDetail, UsageCostDetail> processUsageCost() {
		System.out.println(">>> processUsageCost()");
		return usageDetail -> {
			UsageCostDetail usageCostDetail = new UsageCostDetail();
			usageCostDetail.setUserId(usageDetail.getUserId());
			usageCostDetail.setCallCost(usageDetail.getDuration() * this.ratePerSecond);
			usageCostDetail.setDataCost(usageDetail.getData() * this.ratePerMB);
			return usageCostDetail;
		};
	}
}
  
#+end_src

In the preceding application, we are providing a bean that returns a
java.util.function.Function that consumes a UsageDetail as input and publishes a UsageCostDetail as ouptut.

*** Configuring the UsageCostProcessor Application

When configuring this processor application, we need to set both the input and output destinations (Kafka topics). By default, Sprig Cloud Stream uses binding names as processUsageCost-in-0 and processUsageCost-out-0 which becomes the topic names unless the application overrides them. However, in our case, as in the producer above, we don't want these defaults but rather we would want to make them more descriptive. We want to use the binding name as input and output and provide custom destinations on them.

In src/main/resources/application.properties , you can add the following properties:

#+begin_src 
spring.cloud.stream.function.bindings.processUsageCost-in-0=input
spring.cloud.stream.function.bindings.processUsageCost-out-0=output
spring.cloud.stream.bindings.input.destination=usage-detail
spring.cloud.stream.bindings.output.destination=usage-cost  
#+end_src

1. The spring.cloud.stream.function.bindings.processUsageCost-in-0 overrides the binding name to input .
1. The spring.cloud.stream.function.bindings.processUsageCost-out-0 overrides the binding name to output .
1. The spring.cloud.stream.bindings.processUsageCost-in-0.destination sets the destination to the usage-detail Kafka topic.
1. The spring.cloud.stream.bindings.processUsageCost-out-0.destination sets the destination to the usage-cost Kafka topic.

*** Building   

Now we can build the Usage Cost Processor application. In the usage-cost-processor directory, use the following command to build the project with Maven:

~mvn clean package~

*** Testing

We can use the same test binder that we used above for testing the supplier.

To unit test the UsageCostProcessor , add the following code in the
UsageCostProcessorApplicationTests class:

#+begin_src 
package io.spring.dataflow.sample.usagecostprocessor;

import java.util.HashMap;
import java.util.Map;

import io.spring.dataflow.sample.UsageCostDetail;
import io.spring.dataflow.sample.UsageDetail;
import org.junit.jupiter.api.Test;
import org.springframework.boot.WebApplicationType;
import org.springframework.boot.builder.SpringApplicationBuilder;
import org.springframework.cloud.stream.binder.test.InputDestination;
import org.springframework.cloud.stream.binder.test.OutputDestination;
import org.springframework.cloud.stream.binder.test.TestChannelBinderConfiguration;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.messaging.Message;
import org.springframework.messaging.MessageHeaders;
import org.springframework.messaging.converter.CompositeMessageConverter;
import org.springframework.messaging.converter.MessageConverter;

import static org.assertj.core.api.Assertions.assertThat;

public class UsageCostProcessorApplicationTests {

	@Test
	public void contextLoads() {
	}

	@Test
	public void testUsageCostProcessor() {
		try (ConfigurableApplicationContext context = new SpringApplicationBuilder(
				TestChannelBinderConfiguration.getCompleteConfiguration(
						UsageCostProcessorApplication.class)).web(WebApplicationType.NONE)
				.run()) {

			InputDestination source = context.getBean(InputDestination.class);

			UsageDetail usageDetail = new UsageDetail();
			usageDetail.setUserId("user1");
			usageDetail.setDuration(30L);
			usageDetail.setData(100L);

			final MessageConverter converter = context.getBean(CompositeMessageConverter.class);
			Map<String, Object> headers = new HashMap<>();
			headers.put("contentType", "application/json");
			MessageHeaders messageHeaders = new MessageHeaders(headers);
			final Message<?> message = converter.toMessage(usageDetail, messageHeaders);

			source.send(message);

			OutputDestination target = context.getBean(OutputDestination.class);
			Message<byte[]> sourceMessage = target.receive(10000);

			final UsageCostDetail usageCostDetail = (UsageCostDetail) converter
					.fromMessage(sourceMessage, UsageCostDetail.class);

			assertThat(usageCostDetail.getCallCost()).isEqualTo(3.0);
			assertThat(usageCostDetail.getDataCost()).isEqualTo(5.0);
		}
	}
}  
#+end_src

1. The contextLoads test case verifies the application starts successfully.
1. The testUsageCostProcessor test case uses the test binder's InputDestination to publish a message which is consumed by the function in the processor. Then we use the OutputDestination to verify that the UsageDetail is property transformed into a UsageCostDetail .

** UsageCostLogger Sink   

1. Create a new Maven project with a Group name of io.spring.dataflow.sample and an Artifact name of usage-cost-logger-kafka .
1. In the Dependencies text box, type kafka to select the Kafka binder dependency.
1. In the Dependencies text box, type cloud stream to select the Spring Cloud Stream dependency.
1. In the Dependencies text box, type Actuator to select the Spring Boot actuator dependency.
1. If your target platform is Cloud Foundry , type Cloud Connectors to select the Spring Cloud Connector dependency.
1. Click the Generate Project button.

*** Business Logic   

Now we can create the business logic for the sink application. To do so:

1. Create a UsageCostDetail class in the io.spring.dataflow.sample.usagecostlogger package with content that resembles UsageCostDetail.java. The UsageCostDetail class contains userId , callCost , and dataCost properties.
1. Create the UsageCostLogger class in the io.spring.dataflow.sample.usagecostlogger package to receive the UsageCostDetail message and log it. The following listing shows the source code:

#+begin_src 
package io.spring.dataflow.sample.usagecostloggerkafka;

import java.util.function.Consumer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import lombok.extern.slf4j.Slf4j;

@Slf4j
public class UsageCostLogger {
	
	/**
	 * Configuring the UsageCostLogger Application:
       When configuring the consumer application, we need to set the input binding destination (a Kafka topic). 
	   By default, the input binding used by Spring Cloud Stream will be process-in-0 (so does the destination name if the application does not override it). 
	   We want to override these to make the sink application work with the above two applications (source and processor).

       See application.properties
	 */
    @Bean
	public Consumer<UsageCostDetail> process() {
		return usageCostDetail -> {
			log.info("usageCostDetail.toString : " + usageCostDetail.toString());
		};
	}	

}
#+end_src
   
Here we have a java.util.function.Consumer bean that consumes a UsageCostDetail and then logs that information.

*** Configuring the UsageCostLogger Application

When configuring the consumer application, we need to set the input binding destination (a Kafka topic). By default, the input binding used by Spring Cloud Stream will be process-in-0 (so does the destination name if the application does not override it). We want to override these to make the sink application work with the above two applications (source and processor).

In src/main/resources/application.properties , you can add them:

#+begin_src 
spring.cloud.stream.function.bindings.process-in-0=input
spring.cloud.stream.bindings.input.destination=usage-cost 
#+end_src

The spring.cloud.stream.function.bindings.process-in-0 property overrides the binding name to input and spring.cloud.stream.bindings.input.destination property sets the destination to the usagecost` Kafka topic.

There are many configuration options that you can choose to extend/override to achieve the desired runtime behavior when using Apache Kafka as the message broker. The Apache Kafkaspecific binder configuration properties are listed in Apache Kafka-binder documentation (https://cloud.spring.io/spring-cloud-static/spring-cloud-stream-binder-kafka/current/reference/html/spring-cloud-stream-binder-kafka.html#_configuration_options)

*** Building

Now we can build the Usage Cost Logger application. In the usage-cost-logger directory, use the following command to build the project with Maven:

~mvn clean package~

*** Testing

To unit test the UsageCostLogger , add the following code in the
UsageCostLoggerApplicationTests class:

#+begin_src 
package io.spring.dataflow.sample.usagecostlogger;

import java.util.HashMap;
import java.util.Map;

import io.spring.dataflow.sample.UsageCostDetail;
import org.awaitility.Awaitility;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.springframework.boot.WebApplicationType;
import org.springframework.boot.builder.SpringApplicationBuilder;
import org.springframework.boot.test.system.CapturedOutput;
import org.springframework.boot.test.system.OutputCaptureExtension;
import org.springframework.cloud.stream.binder.test.InputDestination;
import org.springframework.cloud.stream.binder.test.TestChannelBinderConfiguration;
import org.springframework.context.ConfigurableApplicationContext;
import org.springframework.messaging.Message;
import org.springframework.messaging.MessageHeaders;
import org.springframework.messaging.converter.CompositeMessageConverter;
import org.springframework.messaging.converter.MessageConverter;

@ExtendWith(OutputCaptureExtension.class)
public class UsageCostLoggerApplicationTests {

	@Test
	public void contextLoads() {
	}

	@Test
	public void testUsageCostLogger(CapturedOutput output) {
		try (ConfigurableApplicationContext context = new SpringApplicationBuilder(
				TestChannelBinderConfiguration
						.getCompleteConfiguration(UsageCostLoggerApplication.class))
				.web(WebApplicationType.NONE)
				.run()) {

			InputDestination source = context.getBean(InputDestination.class);

			UsageCostDetail usageCostDetail = new UsageCostDetail();
			usageCostDetail.setUserId("user1");
			usageCostDetail.setCallCost(3.0);
			usageCostDetail.setDataCost(5.0);

			final MessageConverter converter = context.getBean(CompositeMessageConverter.class);
			Map<String, Object> headers = new HashMap<>();
			headers.put("contentType", "application/json");
			MessageHeaders messageHeaders = new MessageHeaders(headers);
			final Message<?> message = converter.toMessage(usageCostDetail, messageHeaders);

			source.send(message);

			Awaitility.await().until(output::getOut, value -> value.contains("{\"userId\": \"user1\", \"callCost\": \"3.0\", \"dataCost\": \"5.0\" }"));
		}
	}
}  
#+end_src

1. The contextLoads test case verifies the application starts successfully.
1. The testUsageCostLogger test case verifies that the process method of UsageCostLogger is invoked. We use the OutputCaptureExtension facility provided by Spring Boot testing infrastructure to verify that the message is logged to the console.

** Deployment   

In this section, we deploy the applications we created earlier to the local machine, to Cloud Foundry, and to Kubernetes.

When you deploy these three applications ( UsageDetailSender , UsageCostProcessor and UsageCostLogger ), the flow of message is as follows:

#+begin_src 
UsageDetailSender -> UsageCostProcessor -> UsageCostLogger  
#+end_src

The UsageDetailSender source application's output is connected to the UsageCostProcessor processor application's input. The UsageCostProcessor application's output is connected to the UsageCostLogger sink application's input.

When these applications run, the Kafka binder binds the applications' output and input boundaries to the corresponding topics in Kafka.

** Local

This section shows how to run the three applications as standalone applications in your local environment.

If you have not already done so, you must download and set up Kafka in your local environment.

After unpacking the downloaded archive, you can start the ZooKeeper and Kafka servers by running the following commands:

#+begin_src 
./bin/zookeeper-server-start.sh config/zookeeper.properties &
./bin/kafka-server-start.sh config/server.properties &  
#+end_src

*** Running the Source

By using the pre-defined configuration properties (along with a unique server port) for UsageDetailSender , you can run the application, as follows:

#+begin_src 
java -jar target/usage-detail-sender-kafka-0.0.1-SNAPSHOT.jar --server.port=9001 &  
#+end_src

Now you can see the messages being sent to the usage-detail Kafka topic by using the Kafka console consumer, as follows:

#+begin_src 
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic usage-detail  
#+end_src

To list the topics, run the following command:

#+begin_src 
./bin/kafka-topics.sh --zookeeper localhost:2181 --list  
#+end_src

*** Running the Processor

By using the pre-defined configuration properties(along with a unique server port) for UsageCostProcessor, you can run the application, as follows:

#+begin_src 
java -jar target/usage-cost-processor-kafka-0.0.1-SNAPSHOT.jar --server.port=9002 &  
#+end_src

With the UsageDetail data in the usage-detail Kafka topic from the UsageDetailSender source application, you can see the UsageCostDetail from the usage-cost Kafka topic, as follows:

#+begin_src 
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic usage-cost  
#+end_src

*** Running the Sink

By using the pre-defined configuration properties (along with a unique server port) for UsageCostLogger, you can run the application, as follows:

#+begin_src 
 java -jar target/usage-cost-logger-kafka-0.0.1-SNAPSHOT.jar --server.port=9003 & 
#+end_src

Now you can see that this application logs the usage cost detail.

** Cloud Foundry

This section walks you through how to deploy the UsageDetailSender, UsageCostProcessor, and UsageCostLogger applications on CloudFoundry.

